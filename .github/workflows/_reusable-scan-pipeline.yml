# _reusable-scan-pipeline.yml
name: Reusable JS Scan Pipeline (Headless Katana + SHA256 dedupe)

on:
  workflow_call:
    inputs:
      environment_name:
        description: 'The target environment name (e.g., atlassian-com)'
        required: true
        type: string
    secrets:
      SESSION_COOKIE:
        required: false
      DISCORD_WEBHOOK_URL:
        required: false

env:
  PARALLEL_WORKERS: 8
  MAX_DOWNLOADS_PER_CHUNK: 2000
  JS_PROBE_THREADS: 25
  DOWNLOAD_CONCURRENCY: 25

jobs:
  discover_urls:
    name: "Discover JS URLs with Headless Katana (${{ inputs.environment_name }})"
    runs-on: ubuntu-latest
    outputs:
      js_count: ${{ steps.count.outputs.count }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Normalize target
        id: set-domain
        run: |
          DOMAIN="${{ inputs.environment_name }}"
          TARGET_DOMAIN=$(echo "$DOMAIN" | sed 's/-com$/.com/')
          echo "target_domain=$TARGET_DOMAIN" >> $GITHUB_OUTPUT

      - name: Setup Go & Node
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'
      - uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Cache Go/Node/Pip & Chrome
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/go-build
            ${{ env.GOPATH }}/pkg/mod
            ${{ env.GOPATH }}/bin
            ~/.npm
            ~/.cache/pip
            /opt/google/chrome
          key: ${{ runner.os }}-tool-cache-v2 # Incremented key for new cache contents

      - name: Install Tools & Google Chrome
        run: |
          # Install Chrome for headless crawling if not cached
          if ! command -v google-chrome-stable &> /dev/null
          then
            echo "Google Chrome not found, installing..."
            wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb -O chrome.deb
            sudo dpkg -i chrome.deb
            sudo apt-get install -f -y # Install dependencies
            rm chrome.deb
          else
            echo "Google Chrome is already installed (from cache)."
          fi
          
          # Install Go-based tools
          go install -v github.com/projectdiscovery/katana/cmd/katana@latest
          go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH

      - name: Crawl with Headless Katana and Probe JS URLs
        id: count
        env:
          SESSION_COOKIE: ${{ secrets.SESSION_COOKIE }}
        run: |
          TARGET_DOMAIN="${{ steps.set-domain.outputs.target_domain }}"
          echo "ðŸš€ Starting headless discovery for: $TARGET_DOMAIN"
          
          # Step 1: Crawl the root domain and its subdomains with headless Katana
          # -cs: Crawl subdomains
          # -headless: Run a full browser to render JS and find dynamic URLs
          # -sc: Use the system-installed Chrome
          # -jc: Only output Javascript file URLs
          echo "Crawling with Headless Katana to find all static and dynamic JS files..."
          KATANA_ARGS=("-u" "$TARGET_DOMAIN" "-cs" "-silent" "-jc" "-d" "3" "-c" "10" "-headless" "-sc")
          if [ -n "$SESSION_COOKIE" ]; then
            KATANA_ARGS+=("-hH" "Cookie: $SESSION_COOKIE")
          fi
          
          # We no longer need grep because -jc already filters for JS URLs
          katana "${KATANA_ARGS[@]}" > candidate_js_urls.txt
          
          # Step 2: Probe candidate URLs to ensure they are live (200 OK)
          echo "Probing candidate JS URLs to verify they are live..."
          cat candidate_js_urls.txt | httpx -silent -mc 200 -threads ${{ env.JS_PROBE_THREADS }} > js_urls.txt

          # Step 3: Final count and output
          COUNT=$(wc -l < js_urls.txt)
          echo "âœ… Found $COUNT live and valid JS URLs using headless discovery."
          echo "count=$COUNT" >> $GITHUB_OUTPUT

      - name: Upload discovered-js-urls artifact
        uses: actions/upload-artifact@v4
        with:
          name: discovered-js-urls
          path: js_urls.txt

  generate_matrix:
    name: "Generate chunks"
    needs: discover_urls
    if: needs.discover_urls.outputs.js_count > 0
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.split.outputs.matrix }}
    steps:
      - name: Download discovered-js-urls
        uses: actions/download-artifact@v4
        with:
          name: discovered-js-urls
          path: .

      - name: Split into chunks
        id: split
        run: |
          PARALLEL=${{ env.PARALLEL_WORKERS }}
          mkdir -p chunks
          total_lines=$(wc -l < js_urls.txt)
          lines_per_chunk=$(( (total_lines + PARALLEL - 1) / PARALLEL ))
          split -d -l $lines_per_chunk js_urls.txt chunks/chunk_
          CHUNKS_JSON=$(ls -1 chunks/ | jq -R -s -c 'split("\n") | map(select(length > 0))')
          echo "matrix=$CHUNKS_JSON" >> $GITHUB_OUTPUT

      - name: Upload chunk files
        uses: actions/upload-artifact@v4
        with:
          name: subdomain-chunks-${{ inputs.environment_name }}-${{ github.run_id }}
          path: chunks/

  process_chunk:
    name: "Process chunk ${{ matrix.chunk_file }}"
    needs: generate_matrix
    if: needs.generate_matrix.outputs.matrix != '[]'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        chunk_file: ${{ fromJson(needs.generate_matrix.outputs.matrix) }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Cache and setup tools
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/go-build
            ${{ env.GOPATH }}/pkg/mod
            ${{ env.GOPATH }}/bin
            ~/.npm
            ~/.cache/pip
          key: ${{ runner.os }}-tool-cache-v2

      - name: Setup Go & Node
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'
      - uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Install runtime tools
        run: |
          npm install -g js-beautify || true

      - name: Download chunk
        uses: actions/download-artifact@v4
        with:
          name: subdomain-chunks-${{ inputs.environment_name }}-${{ github.run_id }}
          path: chunks/

      - name: Prepare chunk file
        run: |
          CHUNK="chunks/${{ matrix.chunk_file }}"
          if [ ! -f "$CHUNK" ]; then
            echo "Chunk not found."
            exit 0
          fi
          mkdir -p js_files_temp/runner_${{ matrix.chunk_file }}
          cp "$CHUNK" js_files_temp/runner_${{ matrix.chunk_file }}/my_chunk.txt
          echo "Runner has $(wc -l < js_files_temp/runner_${{ matrix.chunk_file }}/my_chunk.txt) URLs."

      - name: Download and process URLs
        env:
          SESSION_COOKIE: ${{ secrets.SESSION_COOKIE }}
        run: |
          set -euo pipefail
          RUNNER_DIR="js_files_temp/runner_${{ matrix.chunk_file }}"
          ALL_URLS="$RUNNER_DIR/my_chunk.txt"
          WGET_ARGS="--user-agent=Mozilla/5.0 --quiet --no-check-certificate --tries=2 --timeout=20"
          if [ -n "${SESSION_COOKIE:-}" ]; then WGET_ARGS="$WGET_ARGS --header=Cookie:${SESSION_COOKIE}"; fi
          LIBS_IGNORE="libs-ignore.txt"
          LIBS_HASHES="libs_hashes.txt"

          process_url() {
            url="$1"
            [ -z "$url" ] && return
            # Layer 1 Filter: URL pattern matching
            if [ -f "$LIBS_IGNORE" ] && grep -qf "$LIBS_IGNORE" <<< "$url"; then
              return
            fi

            tmpf=$(mktemp)
            if wget $WGET_ARGS -O "$tmpf" "$url"; then
              if [ ! -s "$tmpf" ]; then
                rm -f "$tmpf"
                return
              fi
              content_hash=$(sha256sum "$tmpf" | awk '{print $1}')
              if [ -z "$content_hash" ]; then
                rm -f "$tmpf"
                return
              fi
              # Layer 2 Filter: Content hash matching
              if [ -f "$LIBS_HASHES" ] && grep -qF "$content_hash" "$LIBS_HASHES"; then
                rm -f "$tmpf"
                return
              fi
              final="$RUNNER_DIR/${content_hash}.js"
              # Only beautify and move if the file doesn't already exist
              if [ ! -f "$final" ]; then
                js-beautify -r "$tmpf" &>/dev/null || true
                mv "$tmpf" "$final"
              else
                rm -f "$tmpf"
              fi
              echo "${content_hash}.js,$url" >> "$RUNNER_DIR/manifest.csv"
            else
              rm -f "$tmpf"
            fi
          }
          export -f process_url WGET_ARGS LIBS_IGNORE LIBS_HASHES
          
          head -n ${{ env.MAX_DOWNLOADS_PER_CHUNK }} "$ALL_URLS" > "$RUNNER_DIR/to_process.txt"
          cat "$RUNNER_DIR/to_process.txt" | xargs -P ${{ env.DOWNLOAD_CONCURRENCY }} --no-run-if-empty -I {} bash -c 'process_url "$@"' _ {}

      - name: Upload processed artifact
        uses: actions/upload-artifact@v4
        with:
          name: processed-js-${{ inputs.environment_name }}-${{ matrix.chunk_file }}-${{ github.run_id }}
          path: js_files_temp/runner_${{ matrix.chunk_file }}/

  consolidate_and_commit:
    name: "Consolidate & Commit (${{ inputs.environment_name }})"
    needs: process_chunk
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout full repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Normalize target
        id: set-domain
        run: |
          DOMAIN="${{ inputs.environment_name }}"
          TARGET_DOMAIN=$(echo "$DOMAIN" | sed 's/-com$/.com/')
          echo "target_domain=$TARGET_DOMAIN" >> $GITHUB_OUTPUT

      - name: Download all processed artifacts
        uses: actions/download-artifact@v4
        with:
          path: all_artifacts/
          pattern: processed-js-*
          if-no-files-found: 'warn'

      - name: Consolidate files, manifests, and add README
        run: |
          TARGET_BASE="js_files/${{ steps.set-domain.outputs.target_domain }}"
          TARGET_FILES="$TARGET_BASE/files"
          TARGET_MANIFEST="$TARGET_BASE/manifest.csv"
          mkdir -p "$TARGET_FILES"
          TMP_MAN="all_artifacts/merged_manifest.tmp"

          find all_artifacts/ -type f -name "*.js" -exec cp -n {} "$TARGET_FILES/" \;
          find all_artifacts/ -type f -name "manifest.csv" -exec cat {} + >> "$TMP_MAN" || true

          if [ -f "$TMP_MAN" ]; then
            touch "$TARGET_MANIFEST"
            cat "$TMP_MAN" "$TARGET_MANIFEST" | sort -u -t, -k2,2 > "${TARGET_MANIFEST}.final"
            mv "${TARGET_MANIFEST}.final" "$TARGET_MANIFEST"
            rm -f "$TMP_MAN"
            echo "Manifest consolidated: $(wc -l < $TARGET_MANIFEST) entries."
          else
            echo "No new manifest entries found."
          fi

      - name: Create README using Here Document
        run: |
          TARGET_BASE="js_files/${{ steps.set-domain.outputs.target_domain }}"
          README_FILE="$TARGET_BASE/README_MANIFEST.md"
          
          # This is the safe way to write multi-line content with special characters
          cat <<'EOF' > "$README_FILE"
          # README - JS Manifest Lookup
          ------------------------------------
          This manifest maps content-hash filenames to original URLs in CSV format:
          `<hash>.js,<original_url>`
          
          To find the original URL for a given changed JS file (example: `9f2a....js`):
          ```bash
          grep '^9f2a' manifest.csv
          

      - name: Commit changes
        id: git_commit
        run: |
          TARGET_BASE="js_files/${{ steps.set-domain.outputs.target_domain }}"
          git config --global user.name "GitHub Action Bot"
          git config --global user.email "action-bot@github.com"
          git add -A "$TARGET_BASE/"
          if git diff --staged --quiet; then
            echo "No changes detected."
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          git commit -m "[JS Scan] Update files for ${{ steps.set-domain.outputs.target_domain }}"
          git pull origin main --rebase
          git push origin main
          echo "changes_detected=true" >> $GITHUB_OUTPUT
          CHANGED_FILES=$(git diff --name-only HEAD^ HEAD -- "$TARGET_BASE/")
          echo "changed_files<<EOF" >> $GITHUB_OUTPUT
          echo "$CHANGED_FILES" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Analyze & notify
        if: steps.git_commit.outputs.changes_detected == 'true'
        env:
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          TARGET_DOMAIN: ${{ steps.set-domain.outputs.target_domain }}
        run: |
          CHANGED_JS=$(echo "${{ steps.git_commit.outputs.changed_files }}" | grep '\.js$' || true)
          if [ -n "$CHANGED_JS" ]; then
            if [ -f "analyze_js.sh" ]; then
              chmod +x analyze_js.sh
              echo "$CHANGED_JS" | ./analyze_js.sh
            fi
          fi
          if [ -f "send_notification.sh" ]; then
            chmod +x send_notification.sh
            ./send_notification.sh
          fi
