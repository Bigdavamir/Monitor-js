# .github/workflows/js-discovery.yml
# FINAL VERSION 4 - Advanced Architecture based on Eddie's insight
# This workflow uses gau to find both static JS and HTML pages to feed into Playwright.
name: JS Monitor (Eddie's Advanced Discovery Workflow)

on:
  workflow_dispatch:
  

permissions:
  contents: write

jobs:
  # JOB 1: Discover all URLs and split them into two categories: static JS and crawl targets.
  discover_urls:
    name: "1. Discover All URL Types"
    runs-on: ubuntu-latest
    outputs:
      static_js_found: ${{ steps.output_handler.outputs.static_js_found }}
      crawl_targets_found: ${{ steps.output_handler.outputs.crawl_targets_found }}
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.21'
      
      - name: Install Tools
        run: |
          go install -v github.com/lc/gau/v2/cmd/gau@latest
          go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH

      - name: Run Gau for Comprehensive Discovery
        id: discovery
        env:
          TARGET_DOMAIN: ${{ vars.TARGET_DOMAIN }}
        run: |
          gau --threads 5 "$TARGET_DOMAIN" > all_urls.txt
          echo "Gau discovered $(wc -l < all_urls.txt) total URLs."

      - name: Create Static JS List (Path A)
        run: |
          grep -i '\.js' all_urls.txt | \
          grep -ivE "\.(css|xml|json|txt|md|jpg|jpeg|png|gif|svg|ico|woff|woff2|pdf|zip)$" | \
          sed 's/[?#].*//' | \
          sort -u | \
          grep '^https://' > sanitized_js_urls.txt
          
          httpx -list sanitized_js_urls.txt -silent -mc 200 -ct -match-string "javascript" > live_static_js.txt
          echo "Found $(wc -l < live_static_js.txt) live static JS URLs."

      - name: Create Crawl Target List (Path B)
        run: |
          # Exclude common asset files to get clean endpoints and HTML pages
          grep -ivE "\.(js|css|xml|json|txt|md|jpg|jpeg|png|gif|svg|ico|woff|woff2|pdf|zip|mp4|mov)$" all_urls.txt | \
          sed 's/[?#].*//' | \
          sort -u > potential_crawl_targets.txt
          
          # Probe targets and select a random sample of 20 live pages to avoid long runtimes
          httpx -list potential_crawl_targets.txt -silent -mc 200 -cl -ct | \
          grep "HTML" | cut -d' ' -f1 | sort -R | head -n 20 > crawl_targets.txt
          echo "Selected $(wc -l < crawl_targets.txt) high-value pages for deep crawling."

      - name: Set Outputs
        id: output_handler
        run: |
          echo "static_js_found=$( [ -s live_static_js.txt ] && echo true || echo false )" >> $GITHUB_OUTPUT
          echo "crawl_targets_found=$( [ -s crawl_targets.txt ] && echo true || echo false )" >> $GITHUB_OUTPUT
      
      - name: Upload Static JS Artifact
        if: steps.output_handler.outputs.static_js_found == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: static-js-list-${{ github.run_id }}
          path: live_static_js.txt

      - name: Upload Crawl Targets Artifact
        if: steps.output_handler.outputs.crawl_targets_found == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: crawl-targets-${{ github.run_id }}
          path: crawl_targets.txt

  # JOB 2: Run headless browser on the discovered crawl targets.
  dynamic_discovery_headless:
    name: "2. Dynamic Discovery via Headless"
    needs: discover_urls
    if: needs.discover_urls.outputs.crawl_targets_found == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Download Crawl Targets
        uses: actions/download-artifact@v4
        with:
          name: crawl-targets-${{ github.run_id }}
      
      - name: Setup Node and Playwright
        run: |
          sudo apt-get update && sudo apt-get install -y nodejs npm
          npm install playwright
          npx playwright install --with-deps

      - name: Crawl Targets and Extract Dynamic JS
        env:
          TARGET_DOMAIN: ${{ vars.TARGET_DOMAIN }}
        run: |
          cat << 'EOF' > crawl.js
          const { chromium } = require('playwright');
          const fs = require('fs');
          const urlsToCrawl = fs.readFileSync('crawl_targets.txt', 'utf-8').split('\n').filter(Boolean);
          const dynamicJsUrls = new Set();
          
          (async () => {
            const browser = await chromium.launch();
            console.log(`Starting to crawl ${urlsToCrawl.length} target(s)...`);
            for (const url of urlsToCrawl) {
              const page = await browser.newPage();
              page.on('request', r => { if (r.url().endsWith('.js')) { dynamicJsUrls.add(r.url().split(/[?#]/)[0]); } });
              try {
                console.log(`Crawling: ${url}`);
                await page.goto(url, { waitUntil: 'networkidle', timeout: 45000 });
              } catch (e) {
                console.error(`Failed to crawl ${url}: ${e.message}`);
              } finally {
                await page.close();
              }
            }
            await browser.close();
            fs.writeFileSync('dynamic_js.txt', Array.from(dynamicJsUrls).join('\n'));
            console.log(`Discovered ${dynamicJsUrls.size} unique dynamic JS URLs.`);
          })();
          EOF
          
          node crawl.js
          # Also crawl the main domain as a fallback
          echo "https://${TARGET_DOMAIN}" >> crawl_targets.txt
          node crawl.js

      - name: Upload Dynamic JS Artifact
        uses: actions/upload-artifact@v4
        with:
          name: dynamic-js-list-${{ github.run_id }}
          path: dynamic_js.txt
          if-no-files-found: ignore
  
  # JOB 3: Consolidate all JS sources and split them into chunks.
  consolidate_and_split:
    name: "3. Consolidate and Split"
    needs: [discover_urls, dynamic_discovery_headless]
    if: always()
    runs-on: ubuntu-latest
    outputs:
      matrix_json: ${{ steps.split_files.outputs.matrix_json }}
      chunk_count: ${{ steps.split_files.outputs.count }}
    steps:
      - name: Download Static JS List
        uses: actions/download-artifact@v4
        with:
          name: static-js-list-${{ github.run_id }}
          path: .
        continue-on-error: true

      - name: Download Dynamic JS List
        uses: actions/download-artifact@v4
        with:
          name: dynamic-js-list-${{ github.run_id }}
          path: .
        continue-on-error: true

      - name: Consolidate All JS URLs
        run: |
          touch live_static_js.txt dynamic_js.txt # Ensure files exist
          cat live_static_js.txt dynamic_js.txt | sort -u > all_live_js.txt
          echo "Total unique JS URLs from all sources: $(wc -l < all_live_js.txt)"

      - name: Split URL List into Chunks
        id: split_files
        run: |
          if [ ! -s all_live_js.txt ]; then
            echo "No JS files found to process."
            echo "count=0" >> $GITHUB_OUTPUT
            echo "matrix_json=[]" >> $GITHUB_OUTPUT
            exit 0
          fi
          mkdir -p url_chunks
          split -l 100 -d -a 2 all_live_js.txt url_chunks/chunk_
          
          CHUNK_COUNT=$(ls url_chunks/ | wc -l)
          echo "Split URLs into $CHUNK_COUNT chunks for parallel processing."
          echo "count=$CHUNK_COUNT" >> $GITHUB_OUTPUT
          
          MATRIX_JSON=$(seq 0 $((CHUNK_COUNT - 1)) | jq -c --slurp .)
          echo "Generated Matrix: $MATRIX_JSON"
          echo "matrix_json=${MATRIX_JSON}" >> $GITHUB_OUTPUT

      - name: Upload URL Chunks
        if: steps.split_files.outputs.count > 0
        uses: actions/upload-artifact@v4
        with:
          name: subdomain-chunks-${{ github.run_id }}
          path: url_chunks/

  # JOB 4: Processes each chunk of URLs in parallel (Unchanged).
  process_js_chunks:
    name: "4. Process Chunk ${{ matrix.chunk_id }}"
    needs: consolidate_and_split
    if: needs.consolidate_and_split.outputs.chunk_count > 0
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        chunk_id: ${{ fromJson(needs.consolidate_and_split.outputs.matrix_json) }}
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Download URL Chunks
        uses: actions/download-artifact@v4
        with:
          name: subdomain-chunks-${{ github.run_id }}
          path: url_chunks

      - name: Run Processing Steps (7-9) on Chunk
        env:
          TARGET_DOMAIN: ${{ vars.TARGET_DOMAIN }}
        run: |
          CHUNK_ID=$(printf "%02d" ${{ matrix.chunk_id }})
          MY_CHUNK_FILE="url_chunks/chunk_${CHUNK_ID}"
          if [ ! -f "$MY_CHUNK_FILE" ]; then echo "Chunk file not found, skipping."; exit 0; fi
          
          while read url; do
            if content=$(curl -s -L --max-time 15 "$url"); then
              if [ -n "$content" ]; then
                hash=$(echo -n "$content" | sha256sum | awk '{print $1}')
                echo "$url,$hash"
              fi
            else
              echo "Warning: Skipping malformed or unreachable URL: $url" >&2
            fi
          done < "$MY_CHUNK_FILE" > current_hashes.csv

          TARGET_DIR="js_files/$TARGET_DOMAIN"
          mkdir -p "$TARGET_DIR"
          KNOWN_HASHES_FILE="$TARGET_DIR/known_hashes.csv"
          touch "$KNOWN_HASHES_FILE"
          cut -d, -f2 "$KNOWN_HASHES_FILE" > known_hashes_only.txt
          grep -F -v -f known_hashes_only.txt current_hashes.csv > new_files_unfiltered.csv

          if [ ! -s new_files_unfiltered.csv ]; then echo "No new files in this chunk."; exit 0; fi

          FINAL_NEW_FILES="final_new_files.csv"
          PATTERNS="jquery|react|vue|angular|bootstrap|moment|lodash|vimeo|polyfill|sdk|sentry"
          while IFS=, read -r url hash; do
            if content=$(curl -s -L --max-time 15 "$url" | head -n 20); then
              if ! echo "$content" | grep -iqE "$PATTERNS"; then
                echo "$url,$hash" >> "$FINAL_NEW_FILES"
              fi
            else
              echo "Warning: Skipping unreachable URL during filter: $url" >&2
            fi
          done < new_files_unfiltered.csv

      - name: Upload Final Chunk Results
        uses: actions/upload-artifact@v4
        with:
          name: final-results-${{ github.run_id }}-${{ matrix.chunk_id }}
          path: final_new_files.csv
          if-no-files-found: ignore

  # JOB 5: Consolidates all results and sends a single notification (Unchanged).
  consolidate_and_notify:
    name: "5. Consolidate and Notify"
    needs: process_js_chunks
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download All Final Results
        uses: actions/download-artifact@v4
        with:
          path: all_results/
          pattern: final-results-${{ github.run_id }}-*
          merge-multiple: true
          if-no-files-found: 'warn' 

      - name: Consolidate and Commit
        id: consolidate
        env:
          TARGET_DOMAIN: ${{ vars.TARGET_DOMAIN }}
        run: |
          cat all_results/*.csv > all_new_files.csv 2>/dev/null || true
          
          if [ ! -s all_new_files.csv ]; then
            echo "No new JS files found across all runners."
            echo "found=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "found=true" >> $GITHUB_OUTPUT
          
          SUMMARY=$(awk -F, '{print "✅ " $1}' "all_new_files.csv")
          echo "summary<<EOF" >> $GITHUB_OUTPUT
          echo -e "$SUMMARY" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          TARGET_DIR="js_files/$TARGET_DOMAIN"
          mkdir -p "$TARGET_DIR"
          KNOWN_HASHES_FILE="$TARGET_DIR/known_hashes.csv"
          touch "$KNOWN_HASHES_FILE"
          git config --global user.name "GitHub Action Bot"
          git config --global user.email "action-bot@github.com"
          cat all_new_files.csv >> "$KNOWN_HASHES_FILE"
          sort -t, -k2 -u -o "$KNOWN_HASHES_FILE" "$KNOWN_HASHES_FILE"
          git add "$KNOWN_HASHES_FILE"
          git commit -m "[JS Scan] Discovered $(wc -l < all_new_files.csv) new JS files for $TARGET_DOMAIN"
          git pull origin main --rebase && git push

      - name: Send Notification
        if: steps.consolidate.outputs.found == 'true'
        env:
          TARGET_DOMAIN: ${{ vars.TARGET_DOMAIN }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          SUMMARY_CONTENT: ${{ steps.consolidate.outputs.summary }}
        run: |
          if [ -n "$DISCORD_WEBHOOK_URL" ]; then
            curl -X POST -H "Content-Type: application/json" \
            -d "{\"content\": \"🔎 **New JS URLs for \`$TARGET_DOMAIN\`**\n\`\`\`\n${SUMMARY_CONTENT}\n\`\`\`\"}" \
            "$DISCORD_WEBHOOK_URL"
          fi
