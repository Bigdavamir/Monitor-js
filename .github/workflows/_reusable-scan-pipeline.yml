# _reusable-scan-pipeline.yml
name: Reusable JS Scan Pipeline (Headless Katana + SHA256 dedupe)

on:
  workflow_call:
    inputs:
      environment_name:
        description: 'The target environment name (e.g., atlassian-com)'
        required: true
        type: string
    secrets:
      SESSION_COOKIE:
        required: false
      DISCORD_WEBHOOK_URL:
        required: false

env:
  PARALLEL_WORKERS: 8
  MAX_DOWNLOADS_PER_CHUNK: 2000
  JS_PROBE_THREADS: 25
  DOWNLOAD_CONCURRENCY: 25

jobs:
  discover_urls:
    runs-on: ubuntu-latest
    outputs:
      js_urls_count: ${{ steps.count.outputs.count }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Normalize target domain name
        id: set-domain
        run: |
          # Removes protocol and trailing slashes for clean directory names
          TARGET_DOMAIN=$(echo "${{ inputs.environment_name }}" | sed -E 's,^https?://,,; s,/$,,')
          echo "target_domain=$TARGET_DOMAIN" >> $GITHUB_OUTPUT

      - name: Setup Go & Node
        uses: actions/setup-node@v4
        with:
          node-version: '18'
      - run: go version
      - run: node --version

      # FIX 3: Correctly cache Go binaries instead of looking for go.sum
      - name: Cache Go/Node/Pip & Chrome
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/go-build
            ~/go/bin
            ~/.npm
            ~/.cache/pip
            ~/.cache/google-chrome
          key: ${{ runner.os }}-go-node-pip-chrome-${{ hashFiles('**/go.mod', '**/go.sum', '**/package-lock.json', '**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-go-node-pip-chrome-

      - name: Install Tools & Google Chrome
        run: |
          sudo apt-get update && sudo apt-get install -y libpcre3-dev
          go install -v github.com/projectdiscovery/katana/cmd/katana@latest
          go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
          wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo apt install -y ./google-chrome-stable_current_amd64.deb
          npm install -g js-beautify
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH
        
      - name: Crawl with Headless Katana and Probe JS URLs
        id: count
        env:
          SESSION_COOKIE: ${{ secrets.SESSION_COOKIE }}
        run: |
          TARGET_DOMAIN="${{ steps.set-domain.outputs.target_domain }}"
          echo "🚀 Starting Headless discovery for: $TARGET_DOMAIN"
          
          # --- DEBUG: Verify Chrome installation before running Katana ---
          echo "Verifying Chrome installation..."
          google-chrome --version

          # Define Katana arguments as a Bash array
          KATANA_ARGS=(
            "-u" "$TARGET_DOMAIN"
            "-v"                # --- DEBUG: Enable verbose output to see internal logs ---
            "-jc"               # Extract JS files only
            "-d" "3"            # Max depth
            "-c" "8"            # Concurrency
            "-headless"
            "-sc"               # Use installed chrome
            "-timeout" "180"
            "-hht" "120"
          )
          
          # Conditionally add the session cookie header if it exists
          if [ -n "$SESSION_COOKIE" ]; then
            KATANA_ARGS+=("-hH" "Cookie: $SESSION_COOKIE")
          fi
          
          # Execute Katana and redirect output. We expect detailed logs now.
          katana "${KATANA_ARGS[@]}" > candidate_js_urls.txt

          echo "Probing candidate JS URLs to verify they are live..."
          cat candidate_js_urls.txt | httpx -silent -mc 200 -threads 50 > js_urls.txt
          
          COUNT=$(wc -l < js_urls.txt)
          echo "✅ Found $COUNT live and valid JS URLs using Headless discovery."
          echo "count=$COUNT" >> $GITHUB_OUTPUT



      - name: Upload discovered-js-urls artifact
        uses: actions/upload-artifact@v4
        with:
          name: discovered-js-urls
          path: js_urls.txt
          if-no-files-found: 'warn'


  generate_matrix:
    name: "Generate chunks"
    needs: discover_urls
    if: needs.discover_urls.outputs.js_count > 0
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.split.outputs.matrix }}
    steps:
      - name: Download discovered-js-urls
        uses: actions/download-artifact@v4
        with:
          name: discovered-js-urls
          path: .

      - name: Split into chunks
        id: split
        run: |
          PARALLEL=${{ env.PARALLEL_WORKERS }}
          mkdir -p chunks
          total_lines=$(wc -l < js_urls.txt)
          lines_per_chunk=$(( (total_lines + PARALLEL - 1) / PARALLEL ))
          split -d -l $lines_per_chunk js_urls.txt chunks/chunk_
          CHUNKS_JSON=$(ls -1 chunks/ | jq -R -s -c 'split("\n") | map(select(length > 0))')
          echo "matrix=$CHUNKS_JSON" >> $GITHUB_OUTPUT

      - name: Upload chunk files
        uses: actions/upload-artifact@v4
        with:
          name: subdomain-chunks-${{ inputs.environment_name }}-${{ github.run_id }}
          path: chunks/

  process_chunk:
    name: "Process chunk ${{ matrix.chunk_file }}"
    needs: generate_matrix
    if: needs.generate_matrix.outputs.matrix != '[]'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        chunk_file: ${{ fromJson(needs.generate_matrix.outputs.matrix) }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Cache and setup tools
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/go-build
            ${{ env.GOPATH }}/pkg/mod
            ${{ env.GOPATH }}/bin
            ~/.npm
            ~/.cache/pip
          key: ${{ runner.os }}-tool-cache-v2

      - name: Setup Go & Node
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'
      - uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Install runtime tools
        run: |
          npm install -g js-beautify || true

      - name: Download chunk
        uses: actions/download-artifact@v4
        with:
          name: subdomain-chunks-${{ inputs.environment_name }}-${{ github.run_id }}
          path: chunks/

      - name: Prepare chunk file
        run: |
          CHUNK="chunks/${{ matrix.chunk_file }}"
          if [ ! -f "$CHUNK" ]; then
            echo "Chunk not found."
            exit 0
          fi
          mkdir -p js_files_temp/runner_${{ matrix.chunk_file }}
          cp "$CHUNK" js_files_temp/runner_${{ matrix.chunk_file }}/my_chunk.txt
          echo "Runner has $(wc -l < js_files_temp/runner_${{ matrix.chunk_file }}/my_chunk.txt) URLs."

      - name: Download and process URLs
        env:
          SESSION_COOKIE: ${{ secrets.SESSION_COOKIE }}
        run: |
          set -euo pipefail
          RUNNER_DIR="js_files_temp/runner_${{ matrix.chunk_file }}"
          ALL_URLS="$RUNNER_DIR/my_chunk.txt"
          WGET_ARGS="--user-agent=Mozilla/5.0 --quiet --no-check-certificate --tries=2 --timeout=20"
          if [ -n "${SESSION_COOKIE:-}" ]; then WGET_ARGS="$WGET_ARGS --header=Cookie:${SESSION_COOKIE}"; fi
          LIBS_IGNORE="libs-ignore.txt"
          LIBS_HASHES="libs_hashes.txt"

          process_url() {
            url="$1"
            [ -z "$url" ] && return
            # Layer 1 Filter: URL pattern matching
            if [ -f "$LIBS_IGNORE" ] && grep -qf "$LIBS_IGNORE" <<< "$url"; then
              return
            fi

            tmpf=$(mktemp)
            if wget $WGET_ARGS -O "$tmpf" "$url"; then
              if [ ! -s "$tmpf" ]; then
                rm -f "$tmpf"
                return
              fi
              content_hash=$(sha256sum "$tmpf" | awk '{print $1}')
              if [ -z "$content_hash" ]; then
                rm -f "$tmpf"
                return
              fi
              # Layer 2 Filter: Content hash matching
              if [ -f "$LIBS_HASHES" ] && grep -qF "$content_hash" "$LIBS_HASHES"; then
                rm -f "$tmpf"
                return
              fi
              final="$RUNNER_DIR/${content_hash}.js"
              # Only beautify and move if the file doesn't already exist
              if [ ! -f "$final" ]; then
                js-beautify -r "$tmpf" &>/dev/null || true
                mv "$tmpf" "$final"
              else
                rm -f "$tmpf"
              fi
              echo "${content_hash}.js,$url" >> "$RUNNER_DIR/manifest.csv"
            else
              rm -f "$tmpf"
            fi
          }
          export -f process_url WGET_ARGS LIBS_IGNORE LIBS_HASHES
          
          head -n ${{ env.MAX_DOWNLOADS_PER_CHUNK }} "$ALL_URLS" > "$RUNNER_DIR/to_process.txt"
          cat "$RUNNER_DIR/to_process.txt" | xargs -P ${{ env.DOWNLOAD_CONCURRENCY }} --no-run-if-empty -I {} bash -c 'process_url "$@"' _ {}

      - name: Upload processed artifact
        uses: actions/upload-artifact@v4
        with:
          name: processed-js-${{ inputs.environment_name }}-${{ matrix.chunk_file }}-${{ github.run_id }}
          path: js_files_temp/runner_${{ matrix.chunk_file }}/

  consolidate_and_commit:
    name: "Consolidate & Commit (${{ inputs.environment_name }})"
    needs: process_chunk
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout full repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Normalize target
        id: set-domain
        run: |
          DOMAIN="${{ inputs.environment_name }}"
          TARGET_DOMAIN=$(echo "$DOMAIN" | sed 's/-com$/.com/')
          echo "target_domain=$TARGET_DOMAIN" >> $GITHUB_OUTPUT

      - name: Download all processed artifacts
        uses: actions/download-artifact@v4
        with:
          path: all_artifacts/
          pattern: processed-js-*
          if-no-files-found: 'warn'

      - name: Consolidate files, manifests, and add README
        run: |
          # --- START: Variables ---
          TARGET_BASE="js_files/${{ steps.set-domain.outputs.target_domain }}"
          TARGET_FILES="$TARGET_BASE/files"
          TARGET_MANIFEST="$TARGET_BASE/manifest.csv"
          TMP_MAN="all_artifacts/merged_manifest.tmp"
          # --- END: Variables ---
          
          mkdir -p "$TARGET_FILES"
          
          # --- CORE FIX: Check if the artifacts directory exists AND is not empty ---
          if [ -d "all_artifacts" ] && [ "$(ls -A all_artifacts)" ]; then
            echo "Artifacts found. Starting consolidation..."
          
            # Move JS files, do not overwrite existing ones
            find all_artifacts/ -type f -name "*.js" -exec cp -n {} "$TARGET_FILES/" \;
          
            # Consolidate all manifest.csv files from all chunks
            find all_artifacts/ -type f -name "manifest.csv" -exec cat {} + > "$TMP_MAN" || true
          
            # If the temporary manifest has content, merge it
            if [ -f "$TMP_MAN" ] && [ -s "$TMP_MAN" ]; then
              touch "$TARGET_MANIFEST" # Ensure the main manifest exists
              
              # Merge, sort by URL (field 2), and remove duplicates
              cat "$TMP_MAN" "$TARGET_MANIFEST" | sort -u -t, -k2,2 > "${TARGET_MANIFEST}.final"
              mv "${TARGET_MANIFEST}.final" "$TARGET_MANIFEST"
              
              rm -f "$TMP_MAN"
              echo "✅ Manifest consolidated: $(wc -l < "$TARGET_MANIFEST") total entries."
            else
              echo "No new manifest entries found in artifacts."
            fi
          
          else
            echo "⚠️ Warning: 'all_artifacts' directory not found or is empty. No new files to process."
          fi
    


      - name: Create README using Here Document
        run: |
          TARGET_BASE="js_files/${{ steps.set-domain.outputs.target_domain }}"
          README_FILE="$TARGET_BASE/README_MANIFEST.md"
          
          # Using <<-EOF allows the closing EOF to be indented
          cat <<-EOF > "$README_FILE"
          # README - JS Manifest Lookup
          ------------------------------------
          This manifest maps content-hash filenames to their original URLs.

          The format is: \`<hash>.js,<original_url>\`

          To find the original URL for a specific file:

              # Example: Find URL for a file starting with '9f2a'
              grep '^9f2a' manifest.csv
          EOF

      - name: Detect and Commit Changes
        id: git_commit
        run: |
          TARGET_BASE="js_files/${{ steps.set-domain.outputs.target_domain }}"
          git config --global user.name "GitHub Action Bot"
          git config --global user.email "action-bot@github.com"
          git add -A "$TARGET_BASE/"
          
          if git diff --staged --quiet; then
            echo "✅ No changes detected in monitored JS files."
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "📦 Changes detected. Committing updates..."
          git commit -m "[JS Scan] Update files for ${{ steps.set-domain.outputs.target_domain }}"
          git pull origin main --rebase
          git push origin main
          
          echo "changes_detected=true" >> $GITHUB_OUTPUT
          
          # Capture changed files for the next step
          CHANGED_FILES=$(git diff --name-only HEAD^ HEAD -- "$TARGET_BASE/")
          echo "changed_files<<EOF" >> $GITHUB_OUTPUT
          echo "$CHANGED_FILES" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT


      - name: Analyze & notify
        if: steps.git_commit.outputs.changes_detected == 'true'
        env:
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          TARGET_DOMAIN: ${{ steps.set-domain.outputs.target_domain }}
        run: |
          CHANGED_JS=$(echo "${{ steps.git_commit.outputs.changed_files }}" | grep '\.js$' || true)
          if [ -n "$CHANGED_JS" ]; then
            if [ -f "analyze_js.sh" ]; then
              chmod +x analyze_js.sh
              echo "$CHANGED_JS" | ./analyze_js.sh
            fi
          fi
          if [ -f "send_notification.sh" ]; then
            chmod +x send_notification.sh
            ./send_notification.sh
          fi
