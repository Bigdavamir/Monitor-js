# .github/workflows/js-discovery.yml
# FINAL VERSION 2 - Handles "No Artifacts Found" scenario
name: JS Monitor (Eddie's 9-Step Parallel Workflow)

on:
  workflow_dispatch:
  schedule:
    - cron: '0 */12 * * *'

permissions:
  contents: write

jobs:
  # JOB 1: Gathers all URLs and splits them into chunks for parallel processing.
  discover_and_split_urls:
    name: "1. Discover and Split URLs"
    runs-on: ubuntu-latest
    outputs:
      matrix_json: ${{ steps.split_files.outputs.matrix_json }}
      chunk_count: ${{ steps.split_files.outputs.count }}
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Tools
        run: |
          go install -v github.com/lc/gau/v2/cmd/gau@latest
          go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH
          sudo apt-get update && sudo apt-get install -y nodejs npm jq
          npm install playwright
          npx playwright install --with-deps

      - name: Run Discovery Steps (1-6)
        id: discovery
        env:
          TARGET_DOMAIN: ${{ vars.TARGET_DOMAIN }}
        run: |
          # Steps 1-4 (GAU, Filter, Sanitize)
          gau --verbose "$TARGET_DOMAIN" | \
          grep -i '\.js' | \
          grep -ivE "\.(css|xml|json|txt|md|jpg|jpeg|png|gif|svg|ico|woff|woff2|pdf|zip)$" | \
          sed 's/[?#].*//' | \
          sort -u | \
          grep '^https://' > sanitized_urls.txt

          # Step 5 (Validate Static JS)
          httpx -list sanitized_urls.txt -silent -mc 200 -ct -match-string "javascript" > live_static_js.txt

          # Step 6 (Discover Dynamic JS & Consolidate)
          cat << 'EOF' > crawl.js
          const { chromium } = require('playwright');
          const fs = require('fs');
          (async () => {
            const browser = await chromium.launch();
            const page = await browser.newPage();
            const dynamicJsUrls = new Set();
            page.on('request', r => { if (r.url().endsWith('.js')) { dynamicJsUrls.add(r.url().split(/[?#]/)[0]); } });
            try { await page.goto(`https://${process.env.TARGET_DOMAIN}`, { waitUntil: 'networkidle', timeout: 45000 }); }
            catch (e) { console.error(`Crawl failed: ${e.message}`); }
            await browser.close();
            fs.writeFileSync('dynamic_js.txt', Array.from(dynamicJsUrls).join('\n'));
          })();
          EOF
          node crawl.js
          cat live_static_js.txt dynamic_js.txt | sort -u > all_live_js.txt
          echo "Total unique JS URLs found: $(wc -l < all_live_js.txt)"

      - name: Split URL List into Chunks
        id: split_files
        run: |
          mkdir -p url_chunks
          # Split into chunks of 100 URLs each
          split -l 100 -d -a 2 all_live_js.txt url_chunks/chunk_
          
          CHUNK_COUNT=$(ls url_chunks/ | wc -l)
          echo "Split URLs into $CHUNK_COUNT chunks for parallel processing."
          echo "count=$CHUNK_COUNT" >> $GITHUB_OUTPUT
          
          if [ "$CHUNK_COUNT" -gt 0 ]; then
            MATRIX_JSON=$(seq 0 $((CHUNK_COUNT - 1)) | jq -c --slurp .)
          else
            MATRIX_JSON="[]"
          fi
          echo "Generated Matrix: $MATRIX_JSON"
          echo "matrix_json=${MATRIX_JSON}" >> $GITHUB_OUTPUT

      - name: Upload URL Chunks
        uses: actions/upload-artifact@v4
        with:
          name: subdomain-chunks-${{ github.run_id }}
          path: url_chunks/

  # JOB 2: Processes each chunk of URLs in parallel.
  process_js_chunks:
    name: "2. Process Chunk ${{ matrix.chunk_id }}"
    needs: discover_and_split_urls
    if: needs.discover_and_split_urls.outputs.chunk_count > 0
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        chunk_id: ${{ fromJson(needs.discover_and_split_urls.outputs.matrix_json) }}
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Download URL Chunks
        uses: actions/download-artifact@v4
        with:
          name: subdomain-chunks-${{ github.run_id }}
          path: url_chunks

      - name: Run Processing Steps (7-9) on Chunk
        id: process
        env:
          TARGET_DOMAIN: ${{ vars.TARGET_DOMAIN }}
        run: |
          CHUNK_ID=$(printf "%02d" ${{ matrix.chunk_id }})
          MY_CHUNK_FILE="url_chunks/chunk_${CHUNK_ID}"
          
          if [ ! -f "$MY_CHUNK_FILE" ]; then echo "Chunk file not found, skipping."; exit 0; fi
          
          # Step 7 & 8 (Hash and find new files)
          while read url; do
            if content=$(curl -s -L --max-time 15 "$url"); then
              if [ -n "$content" ]; then
                hash=$(echo -n "$content" | sha256sum | awk '{print $1}')
                echo "$url,$hash"
              fi
            else
              echo "Warning: Skipping malformed or unreachable URL: $url" >&2
            fi
          done < "$MY_CHUNK_FILE" > current_hashes.csv

          TARGET_DIR="js_files/$TARGET_DOMAIN"
          mkdir -p "$TARGET_DIR"
          KNOWN_HASHES_FILE="$TARGET_DIR/known_hashes.csv"
          touch "$KNOWN_HASHES_FILE"
          cut -d, -f2 "$KNOWN_HASHES_FILE" > known_hashes_only.txt
          grep -F -v -f known_hashes_only.txt current_hashes.csv > new_files_unfiltered.csv

          if [ ! -s new_files_unfiltered.csv ]; then echo "No new files in this chunk."; exit 0; fi

          # Step 9 (Filter Libraries)
          FINAL_NEW_FILES="final_new_files.csv"
          PATTERNS="jquery|react|vue|angular|bootstrap|moment|lodash|vimeo|polyfill|sdk|sentry"
          while IFS=, read -r url hash; do
            if content=$(curl -s -L --max-time 15 "$url" | head -n 20); then
              if ! echo "$content" | grep -iqE "$PATTERNS"; then
                echo "$url,$hash" >> "$FINAL_NEW_FILES"
              fi
            else
              echo "Warning: Skipping unreachable URL during filter: $url" >&2
            fi
          done < new_files_unfiltered.csv

      - name: Upload Final Chunk Results
        uses: actions/upload-artifact@v4
        with:
          name: final-results-${{ github.run_id }}-${{ matrix.chunk_id }}
          path: final_new_files.csv
          if-no-files-found: ignore

  # JOB 3: Consolidates all results and sends a single notification.
  consolidate_and_notify:
    name: "3. Consolidate and Notify"
    needs: process_js_chunks
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download All Final Results
        uses: actions/download-artifact@v4
        with:
          name: final-results-${{ github.run_id }}
          path: all_results/
          pattern: final-results-${{ github.run_id }}-*
          merge-multiple: true
          # --- THIS IS THE FIX ---
          if-no-files-found: 'warn' 

      - name: Consolidate and Commit
        id: consolidate
        env:
          TARGET_DOMAIN: ${{ vars.TARGET_DOMAIN }}
        run: |
          cat all_results/*.csv > all_new_files.csv 2>/dev/null || true
          
          if [ ! -s all_new_files.csv ]; then
            echo "No new JS files found across all runners."
            echo "found=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "found=true" >> $GITHUB_OUTPUT
          
          SUMMARY=$(awk -F, '{print "âœ… " $1}' "all_new_files.csv")
          echo "summary<<EOF" >> $GITHUB_OUTPUT
          echo -e "$SUMMARY" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          TARGET_DIR="js_files/$TARGET_DOMAIN"
          mkdir -p "$TARGET_DIR"
          KNOWN_HASHES_FILE="$TARGET_DIR/known_hashes.csv"
          touch "$KNOWN_HASHES_FILE"
          git config --global user.name "GitHub Action Bot"
          git config --global user.email "action-bot@github.com"
          cat all_new_files.csv >> "$KNOWN_HASHES_FILE"
          sort -t, -k2 -u -o "$KNOWN_HASHES_FILE" "$KNOWN_HASHES_FILE"
          git add "$KNOWN_HASHES_FILE"
          git commit -m "[JS Scan] Discovered $(wc -l < all_new_files.csv) new JS files for $TARGET_DOMAIN"
          git pull origin main --rebase && git push

      - name: Send Notification
        if: steps.consolidate.outputs.found == 'true'
        env:
          TARGET_DOMAIN: ${{ vars.TARGET_DOMAIN }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          SUMMARY_CONTENT: ${{ steps.consolidate.outputs.summary }}
        run: |
          if [ -n "$DISCORD_WEBHOOK_URL" ]; then
            curl -X POST -H "Content-Type: application/json" \
            -d "{\"content\": \"ðŸ”Ž **New JS URLs for \`$TARGET_DOMAIN\`**\n\`\`\`\n${SUMMARY_CONTENT}\n\`\`\`\"}" \
            "$DISCORD_WEBHOOK_URL"
          fi
