# .github/workflows/eddie_9_step_js_monitor.yml
name: JS Monitor (Eddie's 9-Step Workflow)

on:
  workflow_dispatch:
  schedule:
    - cron: '0 */12 * * *'

permissions:
  contents: write

jobs:
  run-9-step-scan:
    name: "Run 9-Step JS Discovery"
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Tools
        run: |
          # Go tools
          go install -v github.com/lc/gau/v2/cmd/gau@latest
          go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH
          # Node.js and Playwright
          sudo apt-get update && sudo apt-get install -y nodejs npm
          npm install playwright
          npx playwright install --with-deps

      - name: Run 9-Step Discovery and Processing Script
        id: discovery
        env:
          TARGET_DOMAIN: ${{ vars.TARGET_DOMAIN }}
        run: |
          # STEP 1: GAU
          echo "Step 1: Running gau..."
          gau --verbose "$TARGET_DOMAIN" > s1.txt

          # STEP 2: Filter for JS
          echo "Step 2: Filtering for JS files..."
          grep -i '\.js' s1.txt | grep -ivE '\.(jpg|jpeg|png|gif|svg|css|txt|ico|woff|woff2)\b' > s2.txt

          # STEP 3: Strip Params & Unique
          echo "Step 3: Stripping parameters and making unique..."
          sed 's/[?#].*//' s2.txt | sort -u > s3.txt

          # STEP 4: Absolute URLs only
          echo "Step 4: Filtering for absolute URLs..."
          grep '^https://' s3.txt > s4.txt

          # STEP 5: Validate Live URLs
          echo "Step 5: Validating live JS files..."
          httpx -list s4.txt -silent -mc 200 -ct -match-string "javascript" > s5_static.txt

          # STEP 6: Headless Discovery
          echo "Step 6: Discovering dynamic JS..."
          cat << 'EOF' > crawl.js
          const { chromium } = require('playwright');
          const fs = require('fs');
          const targetDomain = process.env.TARGET_DOMAIN;
          const dynamicJsUrls = new Set();
          (async () => {
            const browser = await chromium.launch();
            const page = await browser.newPage();
            page.on('request', r => { if (r.url().endsWith('.js')) { dynamicJsUrls.add(r.url().split(/[?#]/)[0]); } });
            try { await page.goto(`https://${targetDomain}`, { waitUntil: 'networkidle', timeout: 30000 }); }
            catch (e) { console.error(`Crawl failed: ${e.message}`); }
            await browser.close();
            fs.writeFileSync('s6_dynamic.txt', Array.from(dynamicJsUrls).join('\n'));
          })();
          EOF
          node crawl.js
          cat s5_static.txt s6_dynamic.txt | sort -u > all_live_js.txt
          
          if [ ! -s all_live_js.txt ]; then echo "No JS files found. Exiting."; exit 0; fi

          # STEP 7: Hashing
          echo "Step 7: Hashing JS content..."
          while read url; do
            content=$(curl -s -L "$url")
            if [ -n "$content" ]; then
              hash=$(echo -n "$content" | sha256sum | awk '{print $1}')
              echo "$url,$hash"
            fi
          done < all_live_js.txt > current_hashes.csv
          
          # STEP 8: Indexing (Finding New Hashes)
          echo "Step 8: Finding new files via hash comparison..."
          TARGET_DIR="js_files/$TARGET_DOMAIN"
          mkdir -p "$TARGET_DIR"
          KNOWN_HASHES_FILE="$TARGET_DIR/known_hashes.csv"
          touch "$KNOWN_HASHES_FILE"
          cut -d, -f2 "$KNOWN_HASHES_FILE" > known_hashes_only.txt
          grep -F -v -f known_hashes_only.txt current_hashes.csv > new_files_unfiltered.csv

          if [ ! -s new_files_unfiltered.csv ]; then echo "No new JS files found. Exiting."; exit 0; fi
          
          # STEP 9: Filter Libraries
          echo "Step 9: Filtering out known libraries..."
          FINAL_NEW_FILES="final_new_files.csv"
          PATTERNS="jquery|react|vue|angular|bootstrap|moment|lodash|vimeo|polyfill|sdk"
          while IFS=, read -r url hash; do
            content=$(curl -s -L "$url" | head -n 20)
            if ! echo "$content" | grep -iqE "$PATTERNS"; then echo "$url,$hash" >> "$FINAL_NEW_FILES"; fi
          done < new_files_unfiltered.csv

          if [ ! -s "$FINAL_NEW_FILES" ]; then
             echo "new_files_found=false" >> $GITHUB_OUTPUT
             echo "No new non-library JS files found."
          else
             echo "new_files_found=true" >> $GITHUB_OUTPUT
             SUMMARY=$(awk -F, '{print "âœ… " $1}' "$FINAL_NEW_FILES")
             echo "summary<<EOF" >> $GITHUB_OUTPUT
             echo -e "$SUMMARY" >> $GITHUB_OUTPUT
             echo "EOF" >> $GITHUB_OUTPUT
          fi
      
      - name: Commit Changes
        if: steps.discovery.outputs.new_files_found == 'true'
        run: |
          TARGET_DIR="js_files/${{ vars.TARGET_DOMAIN }}"
          KNOWN_HASHES_FILE="$TARGET_DIR/known_hashes.csv"
          git config --global user.name "GitHub Action Bot"
          git config --global user.email "action-bot@github.com"
          cat final_new_files.csv >> "$KNOWN_HASHES_FILE"
          sort -t, -k2 -u -o "$KNOWN_HASHES_FILE" "$KNOWN_HASHES_FILE"
          git add "$KNOWN_HASHES_FILE"
          git commit -m "[JS Scan] Discovered $(wc -l < final_new_files.csv) new JS files for ${{ vars.TARGET_DOMAIN }}"
          git pull origin main --rebase && git push

      - name: Send Notification
        if: steps.discovery.outputs.new_files_found == 'true'
        env:
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          SUMMARY_CONTENT: ${{ steps.discovery.outputs.summary }}
        run: |
          if [ -n "$DISCORD_WEBHOOK_URL" ]; then curl -X POST -H "Content-Type: application/json" -d "{\"content\": \"ðŸ”Ž **New JS URLs for `${{ vars.TARGET_DOMAIN }}`**\n\`\`\`\n${SUMMARY_CONTENT}\n\`\`\`\"}" "$DISCORD_WEBHOOK_URL"; fi
