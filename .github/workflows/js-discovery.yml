# .github/workflows/playwright-js-monitor.yml
# FINAL AND CORRECTED VERSION
name: JS Monitor - Playwright Edition

on:
  workflow_dispatch:
  schedule:
    - cron: '0 */12 * * *'
  push:
    branches:
      - 'main'

# FIX: Grant write permissions to the GITHUB_TOKEN for this workflow
permissions:
  contents: write
jobs:
  discover-and-filter:
    name: "1. Discover & Filter URLs (Hybrid Strategy)"
    runs-on: ubuntu-latest
    outputs:
      live_urls_found: ${{ steps.check_file.outputs.found }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.21'
      - name: Install Discovery Tools
        run: |
          go install -v github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest
          go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
          go install -v github.com/lc/gau/v2/cmd/gau@latest
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH
      - name: "Hybrid Discovery: Subdomains + Deep URL Crawl"
        run: |
          # Step 1: Discover all live subdomains
          subfinder -d "${{ vars.TARGET_DOMAIN }}" -silent | httpx -silent -mc 200,301,302,307 > live_subdomains.txt
          
          # Step 2: Discover all historical URLs for the root domain
          gau --providers wayback,commoncrawl,otx,urlscan "${{ vars.TARGET_DOMAIN }}" > gau_urls.txt
          
          # Step 3: Combine lists and filter out static/library content
          cat live_subdomains.txt gau_urls.txt | sort -u | \
          grep -Eiv "\.(jpg|jpeg|png|gif|bmp|svg|webp|css|woff|woff2|ttf|eot|ico|pdf|zip|gz|rar|mp3|mp4|mov|avi)$" | \
          grep -Eiv "/(vendor|assets|lib|libs|third-party|node_modules)/" > potential_targets.txt
          
          # Step 4: Final check to ensure all targets are live
          cat potential_targets.txt | httpx -silent -mc 200,301,302,307 -o live_urls.txt

      - name: Check if live URLs were found
        id: check_file
        run: |
          if [ -s live_urls.txt ]; then
            echo "found=true" >> $GITHUB_OUTPUT
            echo "Found $(wc -l < live_urls.txt) live URLs after filtering."
          else
            echo "found=false" >> $GITHUB_OUTPUT
            echo "No live URLs found. Stopping workflow."
          fi
      - name: Upload live URLs artifact
        if: steps.check_file.outputs.found == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: live-urls-${{ github.run_id }}
          path: live_urls.txt



  generate-matrix:
    name: "2. Generate Parallel Matrix"
    needs: discover-and-filter
    if: needs.discover-and-filter.outputs.live_hosts_found == 'true'
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.split.outputs.matrix }}
    steps:
      - name: Download live subdomains artifact
        uses: actions/download-artifact@v4
        with:
          name: live-subdomains-${{ github.run_id }}
      - name: Split subdomains into chunks
        id: split
        run: |
          mkdir -p chunks
          split -d -l 25 live_subdomains.txt chunks/chunk_
          # Creates a JSON array of chunk filenames, e.g., ["chunk_00", "chunk_01"]
          MATRIX_JSON=$(ls -1q chunks/ | jq -R -s -c 'split("\n") | map(select(length > 0))')
          echo "matrix=${MATRIX_JSON}" >> $GITHUB_OUTPUT
      - name: Upload subdomain chunks artifact
        uses: actions/upload-artifact@v4
        with:
          name: subdomain-chunks-${{ github.run_id }}
          path: chunks/

  extract-js:
    name: "3. Extract JS (Chunk ${{ matrix.chunk_file }})"
    runs-on: ubuntu-latest
    needs: generate-matrix
    strategy:
      fail-fast: false
      matrix:
        chunk_file: ${{ fromJson(needs.generate-matrix.outputs.matrix) }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
      - name: Download subdomain chunk
        uses: actions/download-artifact@v4
        with:
          name: subdomain-chunks-${{ github.run_id }}
          # FIX: Download the artifact *into* a 'chunks' directory
          path: chunks
      - name: Install Playwright and dependencies
        run: |
          npm install playwright
          npx playwright install --with-deps chromium
      - name: Run Playwright JS Extraction
        run: |
          CHUNK_FILE="chunks/${{ matrix.chunk_file }}"
          echo "Processing chunk file: $CHUNK_FILE"
          
          if [ ! -s "$CHUNK_FILE" ]; then
            echo "Chunk file is empty or does not exist. Skipping."
            exit 0
          fi
          
          while IFS= read -r host; do
            if [ -n "$host" ]; then
              echo "Crawling: $host"
              node extract-js.js "$host"
            fi
          done < "$CHUNK_FILE"
      - name: Upload Captured JS Files
        uses: actions/upload-artifact@v4
        with:
          name: captured-js-${{ matrix.chunk_file }}-${{ github.run_id }}
          path: captured_js/
          if-no-files-found: ignore


  consolidate-and-commit:
    name: "4. Consolidate, Deduplicate & Commit"
    needs: extract-js
    if: always() # Run this job even if some extraction jobs fail
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Full Git History
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Download all JS artifacts
        uses: actions/download-artifact@v4
        with:
          path: all_js_artifacts/
          # FIX: The pattern now correctly matches the uploaded artifact names
          pattern: captured-js-*-${{ github.run_id }}
          if-no-files-found: warn

      - name: Consolidate, Deduplicate, Filter, and Prepare Commit
        id: process_files
        run: |
          TARGET_DIR="js_files/${{ vars.TARGET_DOMAIN }}"
          mkdir -p "$TARGET_DIR"
          MASTER_INDEX_FILE="$TARGET_DIR/index.csv"
          touch "$MASTER_INDEX_FILE"
          
          NEW_FILES_SUMMARY="new_files_summary.log"
          
          # Check if any artifacts were downloaded
          if [ ! -d "all_js_artifacts" ] || [ -z "$(ls -A all_js_artifacts)" ]; then
            echo "No JS files were captured in this run."
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # NEW: Define common library signatures. This list can be expanded.
          PATTERNS="/*! jQuery|jQuery Foundation|React v|@license|Licensed under MIT|Bootstrap v|Angular v|webpackBootstrap|Moment.js|lodash|core-js|modernizr"

          find all_js_artifacts -type f -name "*.js" | while read -r js_file; do
            HASH=$(basename "$js_file" .js)
            
            # Check if this hash is already known
            if ! grep -q "^$HASH," "$MASTER_INDEX_FILE"; then
              
              # --- NEW: Content-based library filtering ---
              # Before processing, check the first 20 lines for library signatures.
              if head -n 20 "$js_file" | grep -qiE "$PATTERNS"; then
                echo "  -> Ignoring new library file: Hash $HASH"
              else
                # This is a new, non-library file. Process it.
                INDEX_FILE=$(dirname "$js_file")/index.txt # Corrected from index.csv to index.txt
                URL=$(grep "^$HASH," "$INDEX_FILE" | cut -d, -f2-)
                
                if [[ -n "$URL" ]]; then
                  echo "New custom file found: Hash $HASH from URL $URL"
                  cp "$js_file" "$TARGET_DIR/$HASH.js"
                  echo "$HASH,$URL,$(date -u +"%Y-%m-%dT%H:%M:%SZ")" >> "$MASTER_INDEX_FILE"
                  echo "âœ… New Custom JS: $URL" >> $NEW_FILES_SUMMARY
                fi
              fi
            fi
          done
          
          if [ ! -s "$NEW_FILES_SUMMARY" ]; then
            echo "No new custom JS files detected."
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "changes_detected=true" >> $GITHUB_OUTPUT
          echo "summary<<EOF" >> $GITHUB_OUTPUT
          cat $NEW_FILES_SUMMARY >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT


      - name: Commit Changes
        if: steps.process_files.outputs.changes_detected == 'true'
        run: |
          git config --global user.name "GitHub Action Bot"
          git config --global user.email "action-bot@github.com"
          
          TARGET_DIR="js_files/${{ vars.TARGET_DOMAIN }}"
          # FIX: Corrected the path for the sort command
          sort -u -o "$TARGET_DIR/index.csv" "$TARGET_DIR/index.csv"
          
          git add "js_files/"
          git commit -m "JS Scan: Update files for ${{ vars.TARGET_DOMAIN }}"
          # Adding retry logic for push, just in case
          git pull origin main --rebase && git push --atomic origin main
          
      - name: Send Notification on Change
        if: steps.process_files.outputs.changes_detected == 'true'
        env:
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          SUMMARY_CONTENT: ${{ steps.process_files.outputs.summary }}
        run: |
          if [ -n "$DISCORD_WEBHOOK_URL" ]; then
            curl -X POST -H "Content-Type: application/json" \
            -d "{\"content\": \"ðŸ”Ž **New JavaScript Files Detected for `${{ vars.TARGET_DOMAIN }}`**\n\`\`\`\n${SUMMARY_CONTENT}\n\`\`\`\"}" \
            "$DISCORD_WEBHOOK_URL"
          else
            echo "Discord webhook URL not set. Skipping notification."
          fi
