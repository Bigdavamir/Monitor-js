# Simplified Reusable Workflow for a Single Target Domain
name: Reusable Scan Pipeline (Single Target)

on:
  workflow_call:
    inputs:
      environment_name:
        required: true
        type: string
    secrets:
      SESSION_COOKIE:
        required: false
      DISCORD_WEBHOOK_URL:
        required: false

jobs:
  # This single job replaces the previous four-job chain.
  scan-and-commit:
    name: "Scan, Consolidate & Notify (${{ inputs.environment_name }})"
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment_name }}
    steps:
      - name: Checkout Full Git History
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Needed for git diff

      - name: Set up Go & Node
        uses: actions/setup-go@v4
        with:
          go-version: '1.21'
      - uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Install Scanning Tools
        run: |
          # Install Go and Node tools
          go install -v github.com/projectdiscovery/katana/cmd/katana@latest
          go install -v github.com/lc/gau/v2/cmd/gau@latest
          npm install -g js-beautify
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH
          
          # Install system analysis dependencies (add git)
          sudo apt-get update && sudo apt-get install -y python3-pip yara git
          
          # Install LinkFinder (this one works correctly with pip)
          pip install git+https://github.com/GerbenJavado/LinkFinder.git
          
          # --- SECRETFINDER FIX ---
          # SecretFinder is not a standard pip package and must be cloned manually.
          echo "Cloning SecretFinder repository..."
          git clone https://github.com/m4ll0k/SecretFinder.git /opt/secretfinder
          echo "Installing SecretFinder dependencies from its requirements.txt..."
          pip install -r /opt/secretfinder/requirements.txt
          echo "Adding SecretFinder directory to the system PATH..."
          echo "/opt/secretfinder" >> $GITHUB_PATH
          # --- END OF FIX ---
          
      - name: Discover and Process JS files
        env:
          SESSION_COOKIE: ${{ secrets.SESSION_COOKIE }}
        run: |
          echo "Scanning single target: ${{ vars.TARGET_DOMAIN }}"
          echo "${{ vars.TARGET_DOMAIN }}" > target.txt

          KATANA_HEADER_ARGS=("-H" "User-Agent: Mozilla/5.0")
          if [ -n "$SESSION_COOKIE" ]; then KATANA_HEADER_ARGS+=("-H" "Cookie: $SESSION_COOKIE"); fi
          
          # Run Katana on the single domain
          katana -list target.txt -jc -silent -d 5 -c 25 "${KATANA_HEADER_ARGS[@]}" -o katana-js.txt || true
          
          # --- GAU FIX ---
          # Run gau and IMMEDIATELY filter its output to keep only the exact target domain.
          # This prevents subdomain URLs from polluting the results.
          echo "Running gau and filtering for exact domain: ${{ vars.TARGET_DOMAIN }}"
          cat target.txt | gau --threads 5 | grep -iE "^https?://(www\.)?${{ vars.TARGET_DOMAIN }}/" > gau-js.txt
          # --- END OF FIX ---

          # Combine, sort, and strictly filter for .js files
          echo "Combining and filtering results for JS files..."
          cat katana-js.txt gau-js.txt | sort -u > all_urls.txt
          
          # Improved JS file filtering
          # Method 1: Get URLs ending in .js (with optional query parameters)
          grep -iE '\.js($|\?)' all_urls.txt > all_js_urls.txt

          # Method 2: Probe remaining URLs to find JS files by Content-Type
          # We need jq for this, so ensure it's installed in the previous step.
          grep -ivE '\.js($|\?)' all_urls.txt | httpx -silent -threads 25 -json -o probed.json || true
          if [ -s probed.json ]; then
            jq -r 'select(.content_type != null and (.content_type | test("javascript|x-javascript|text/javascript|application/ecmascript"))) .url' probed.json >> all_js_urls.txt
          fi

          # Final deduplication
          sort -u -o all_js_urls.txt all_js_urls.txt

          echo "Found $(wc -l < all_js_urls.txt) unique JS URLs to process."
          OUTPUT_DIR="js_files_temp"
          mkdir -p "$OUTPUT_DIR"
          
          WGET_ARGS="--user-agent=Mozilla/5.0 --quiet --no-check-certificate --tries=2 --timeout=20 --dns-timeout=5 --read-timeout=15"
          if [ -n "$SESSION_COOKIE" ]; then WGET_ARGS="$WGET_ARGS --header=Cookie:$SESSION_COOKIE"; fi

          process_url() {
            url="$1"
            if [ -z "$url" ]; then return; fi
            hashed_filename=$(echo -n "$url" | md5sum | awk '{print $1}').js
            JS_FILE_PATH="$OUTPUT_DIR/$hashed_filename"

            if wget $WGET_ARGS -O "$JS_FILE_PATH" "$url"; then
              if [ -s "$JS_FILE_PATH" ]; then
                js-beautify -r "$JS_FILE_PATH" || echo "⚠️ js-beautify failed on $JS_FILE_PATH"
                echo "$hashed_filename,$url" >> "$OUTPUT_DIR/index.txt"
              else
                rm -f "$JS_FILE_PATH"
              fi
            fi
          }
          export -f process_url; export WGET_ARGS; export OUTPUT_DIR
          cat all_js_urls.txt | xargs -P 25 --no-run-if-empty -I {} bash -c 'process_url "$@"' _ {}


      # In .github/workflows/_reusable-scan-pipeline.yml
# Inside the job: 'consolidate-and-commit'

      - name: Consolidate and Commit Changes
        id: commit
        run: |
          echo "Consolidating JS files from all runners..."
          # Move all downloaded files and their index files to the final destination
          find js_files_temp -name "*.js" -exec mv {} js_files/${{ vars.TARGET_DOMAIN }}/ \;
          find js_files_temp -name "index.txt" -exec cat {} + >> js_files/${{ vars.TARGET_DOMAIN }}/index.txt \;
          
          # Clean up the temp directory
          rm -rf js_files_temp

          # Configure Git user
          git config --global user.name "GitHub Action Bot"
          git config --global user.email "action-bot@github.com"

          # Check for changes
          if [[ -z $(git status --porcelain) ]]; then
            echo "No changes detected. Nothing to commit."
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "Changes detected. Preparing to commit."
          echo "changes_detected=true" >> $GITHUB_OUTPUT

          git add js_files/${{ vars.TARGET_DOMAIN }}

          # Check again if add resulted in staged changes
          if git diff --staged --quiet; then
            echo "No effective changes to commit after adding files."
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          git commit -m "[JS Scan] Update files for ${{ vars.TARGET_DOMAIN }}" -m "Scan triggered by ${{ github.event_name }} for ${{ vars.TARGET_DOMAIN }}."

          # --- KEY FIX: Pull latest changes with rebase before pushing ---
          # This syncs the local repo with the remote, avoiding the rejection error.
          echo "Pulling latest changes from remote..."
          git pull --rebase

          echo "Pushing changes to remote..."
          git push


      - name: Analyze Changed Files & Notify
        if: steps.git_commit.outputs.changes_detected == 'true'
        env:
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          TARGET_DOMAIN: ${{ vars.TARGET_DOMAIN }}
        run: |
          CHANGED_JS_FILES=$(echo "${{ steps.git_commit.outputs.changed_files }}" | grep '\.js$')
          if [ -z "$CHANGED_JS_FILES" ]; then
            echo "✅ No changed JS files to analyze."
          else
            echo "🔬 Analyzing changed JS files..."
            chmod +x analyze_js.sh
            echo "$CHANGED_JS_FILES" | ./analyze_js.sh
          fi

          echo "🚀 Notifying about changes..."
          chmod +x send_notification.sh
          ./send_notification.sh
